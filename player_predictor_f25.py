# -*- coding: utf-8 -*-
"""player_predictor_f25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z4SO6rOJz7nuSwYjFfMAyKcIV-VY6qTh
"""

'''
Sample player names to test:
Cade Cunningham
Lebron James
Stephen Curry
Kevin Durant
Josh Giddey
Nikola Jokic
Anthony Edwards
Derrick White
Payton Pritchard
Cooper Flagg
Jaylen Wells
Jalen Brunson
Pascal Siakam
Jamal Murray
Tyler Herro

For opposing team,
only enter the
city. Dictionary
below has all teams
with corresponding
city
'''

current_season = 2026
player_name = input("Enter Player Name: ")
print("\nFor opponent, enter city only.\nA list of all teams for reference is below.\n")
current_opponent = input("Enter today's opponent: ")

import pandas as pd
import numpy as np
import requests

ABBR_TO_TEAM = {
    "ATL": "Atlanta",
    "BOS": "Boston",
    "BRK": "Brooklyn",
    "CHO": "Charlotte",
    "CHI": "Chicago",
    "CLE": "Cleveland",
    "DAL": "Dallas",
    "DEN": "Denver",
    "DET": "Detroit",
    "GSW": "Golden State",
    "HOU": "Houston",
    "IND": "Indiana",
    "LAC": "LA Clippers",
    "LAL": "LA Lakers",
    "MEM": "Memphis",
    "MIA": "Miami",
    "MIL": "Milwaukee",
    "MIN": "Minnesota",
    "NOP": "New Orleans",
    "NYK": "New York",
    "OKC": "Oklahoma City",
    "ORL": "Orlando",
    "PHI": "Philadelphia",
    "PHO": "Phoenix",
    "POR": "Portland",
    "SAC": "Sacramento",
    "SAS": "San Antonio",
    "TOR": "Toronto",
    "UTA": "Utah",
    "WAS": "Washington"
}


FEATURE_COLUMNS = [
    # player's offensive stats
    "MP", "FG", "FGA",
    "FG%", "3P", "3PA", "3P%",
    "2P", "2PA", "2P%",
    "eFG%",
    "FT", "FTA", "FT%",
    "ORB", "DRB", "TRB",
    "AST", "TOV"
]

DEF_FEATURE_COLUMNS = [
    # opposing team's defensive stats
    "oPPG",
    "dEFF",
    "pDIFF",
    "eDIFF",
    "PACE",
    "eWIN%"
]

def make_player_id(name: str):
  # splitting player name into url-friendly split for our data site
    first, last = name.lower().split()
    return f"{last[:5]}{first[:2]}01", last[0]

def make_weights(n, min_w=0.5, max_w=1.5):
  # weighting recent games heavier than early ones
    return np.linspace(min_w, max_w, n)

def scrape_bref_gamelog(name: str, year: int):
  # get the url, and get the all games from given season.
  # there are breaks with the value 'Rk' on the site,
  # so this skips that and combines all games into one table
  # returns the final dataset of the season
    player_id, letter = make_player_id(name)
    url = f"https://www.basketball-reference.com/players/{letter}/{player_id}/gamelog/{year}"
    print(url)
    try:
        tables = pd.read_html(url)

        game_log_dfs = []
        for table in tables:
            if 'Rk' in table.columns and 'Date' in table.columns and 'PTS' in table.columns:
                df_clean = table[table['Rk'] != 'Rk'].copy()
                df_clean = df_clean[df_clean['Rk'].notna()].copy()
                if len(df_clean) > 0:
                    df_clean["Season"] = year
                    game_log_dfs.append(df_clean)
        if game_log_dfs:
            df = pd.concat(game_log_dfs, ignore_index=True)
            df = df.iloc[::-1].reset_index(drop=True)
            return df
    except Exception as e:
        print(f"Failed with pandas.read_html: {e}")
    return None

def get_last_three_seasons(name: str, current_season: int):
  # gets player stats from the last three seasons
    dfs = {}
    for season in [current_season, current_season - 1, current_season - 2]:
        df = scrape_bref_gamelog(name, season)
        if df is not None:
            dfs[season] = df
    if not dfs:
        return None
    return dfs

def scrape_team_defense(year):
  # gets the table of all team defense for given year
    url = f"https://www.nbastuffer.com/{year-1}-{year}-nba-team-stats/"

    try:
        tables = pd.read_html(url)
        if tables:
            return tables[1]
    except Exception as e:
        print(f"Failed with pandas.read_html: {e}")
    return None

def get_last_three_def_seasons(current_season):
  # gets team defense stats from the last three seasons
    dfs = {}
    for season in [current_season, current_season - 1, current_season - 2]:
        df = scrape_team_defense(season)
        if df is not None:
            dfs[season] = df
    if not dfs:
        return None
    return dfs

def min_percentage(mp):
  # turning minutes format into minute percent
    if ":" in mp:
        m, s = mp.split(":")
        return float(m) + float(s) / 60
    return None

df_def = get_last_three_def_seasons(current_season)
df_off = get_last_three_seasons(player_name, current_season)
merged_seasons = []
y = []

# for cases where nan is present in the table
PCT_COLS = ["FG%", "2P%", "3P%", "FT%", "eFG%"]

for season in df_off.keys():
  # combining defense and offense into one df
  # by matching who they played to that team's defense
    off_df = df_off[season].copy()
    off_df["Opp"] = off_df["Opp"].map(ABBR_TO_TEAM)
    off_df["MP"] = off_df["MP"].map(min_percentage)

    for c in PCT_COLS:
        if c in off_df.columns:
            off_df[c] = pd.to_numeric(off_df[c], errors="coerce").fillna(0.0)

    def_df = df_def[season].copy()
    def_df = def_df.rename(columns={"TEAM": "Opp"})
    def_df = def_df[["Opp"] + DEF_FEATURE_COLUMNS]

    merged = off_df.merge(def_df, on="Opp", how="left")
    merged["Season"] = season

    merged_seasons.append(merged)

# remove cases where player did not play,
# they were injured, or was not active
full_df = pd.concat(merged_seasons, ignore_index=True)
full_df = full_df[full_df["MP"].notna()]
full_df = full_df[full_df["MP"] != "Inactive"]
full_df = full_df[full_df["MP"] != "Did Not Play"]

# label all data found into one df X
# remove all cases where they played
# less than 10 minutes to remove noise
X = pd.DataFrame(full_df[FEATURE_COLUMNS + DEF_FEATURE_COLUMNS])
X = X[X["MP"] >= 10]

# make y points scored
y = pd.DataFrame(full_df['PTS'])
y.apply(pd.to_numeric, errors="coerce")
y = y.loc[X.index]

X = X.apply(pd.to_numeric, errors="coerce")
X = X.fillna(0.0)

y = y.apply(pd.to_numeric, errors="coerce")

pd.set_option("display.max_columns", None)
pd.set_option("display.width", None)
display(X)
display(y)

def standardize(X_train, X_test):
    # Calculate the mean and standard deviation of the training set
    mean = np.mean(X_train, axis=0)
    std = np.std(X_train, axis=0)

    # To prevent division by zero, set std=1 where std is zero
    std[std == 0] = 1

    # Standardize the training set
    X_train_scaled = (X_train - mean) / std

    # Use the training set's mean and std to standardize the test set
    X_test_scaled = (X_test - mean) / std

    return X_train_scaled, X_test_scaled, mean, std

def gd(X, y, weights, learning_rate=0.01, n_iterations=1000, alpha=0.1):
  # apply gradient descent to train the dataset
    m, n = X.shape
    theta = np.random.randn(n, 1)

    y = y.reshape(-1, 1)
    weights = weights.reshape(-1, 1)

    mse_history = []

    for iteration in range(n_iterations):
        # Apply sample weights
        residuals = X @ theta - y
        weighted_residuals = weights * residuals

        # Full-batch weighted gradient
        gradients = (2 / m) * X.T @ weighted_residuals

        # Ridge regularization (DO NOT penalize intercept)
        ridge = 2 * alpha * theta
        ridge[0] = 0
        gradients += ridge

        # Update
        theta -= learning_rate * gradients

        # Track MSE (unweighted, for interpretability)
        mse = np.mean((X @ theta - y) ** 2)
        mse_history.append(mse)

        if iteration % 100 == 0 and iteration != 0:
            print(f"Iteration {iteration}: MSE = {mse:.4f}")

    return theta, mse_history

from sklearn.model_selection import KFold

# prep into numpy for kfold
X_np = X.values.astype(float)
y_np = y.values.astype(float).reshape(-1, 1)
weights_full = make_weights(len(X_np))

kf = KFold(n_splits=5, shuffle=False)

train_losses = []
val_losses = []

for fold, (train_idx, val_idx) in enumerate(kf.split(X_np)):
    X_train, X_val = X_np[train_idx], X_np[val_idx]
    y_train, y_val = y_np[train_idx], y_np[val_idx]
    w_train = weights_full[train_idx]

    # Standardize
    X_train_s, X_val_s, mean, std = standardize(X_train, X_val)

    # Add intercept
    X_train_i = np.column_stack([np.ones(len(X_train_s)), X_train_s])
    X_val_i   = np.column_stack([np.ones(len(X_val_s)), X_val_s])

    theta, _ = gd(
        X_train_i,
        y_train,
        w_train,
        learning_rate=0.01,
        n_iterations=500,
        alpha=0.3
    )

    train_mse = np.mean((X_train_i @ theta - y_train) ** 2)
    val_mse   = np.mean((X_val_i @ theta - y_val) ** 2)

    train_losses.append(train_mse)
    val_losses.append(val_mse)

    print(f"Fold {fold+1}: Train MSE = {train_mse:.2f}, Val MSE = {val_mse:.2f}")

print("Avg Train MSE:", np.mean(train_losses))
print("Avg Val MSE:", np.mean(val_losses))

# Final standardization
mean = X_np.mean(axis=0)
std = X_np.std(axis=0)
std[std == 0] = 1

X_scaled = (X_np - mean) / std
X_intercept = np.column_stack([np.ones(len(X_scaled)), X_scaled])

# set weight
weights_final = make_weights(len(X_np))

# and train for final feature strength
theta_final, history = gd(
    X_intercept,
    y_np,
    weights_final,
    learning_rate=0.01,
    n_iterations=600,
    alpha=0.3
)

avg = X.mean()

# get the opposing team's stats for input
opp_def = df_def[current_season].rename(columns={"TEAM": "Opp"}).set_index("Opp")

# filter by used def features
for col in DEF_FEATURE_COLUMNS:
    avg[col] = opp_def.loc[current_opponent, col]

# scale everything
player_avg_scaled = (avg.values - mean) / std
player_avg_intercept = np.insert(player_avg_scaled, 0, 1)

# apply theta to player for final prediction
prediction = player_avg_intercept @ theta_final
val_rmse = np.sqrt(np.mean(val_losses))
pred_pts = prediction.item()
lower = pred_pts - val_rmse
upper = pred_pts + val_rmse

print("Predicted points:", round(pred_pts, 2))
print("\nConclusion:")
print(f"- Expected points: {int(round(pred_pts))}")
print(f"- Typical error: Â±{round(val_rmse, 2)} points")
print(f"- Likely range: [{int(lower)}, {int(upper)}]")